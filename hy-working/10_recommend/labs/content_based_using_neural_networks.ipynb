{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Filtering Using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab relies on files created in the [content_based_preproc.ipynb](./content_based_preproc.ipynb) notebook. Be sure to complete the TODOs in that notebook and run the code there before completing this lab.  \n",
    "Also, we'll be using the **python3** kernel from here on out so don't forget to change the kernel if it's still python2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab illustrates:\n",
    "1. how to build feature columns for a model using tf.feature_column\n",
    "2. how to create custom evaluation metrics and add them to Tensorboard\n",
    "3. how to train a model and make predictions with the saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Hub should already be installed. You can check using pip freeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard==1.8.0\n",
      "tensorflow==1.8.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If 'tensorflow-hub' isn't one of the outputs above, then you'll need to install it. Uncomment the cell below and execute the commands. After doing the pip install, click **\"Reset Session\"** on the notebook so that the Python environment picks up the new packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-hub) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-hub) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-hub) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.4.0->tensorflow-hub) (40.2.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.4.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0421 12:26:08.532776 140294530184960 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import shutil\n",
    "\n",
    "PROJECT = 'qwiklabs-gcp-2737edcaba61fe20' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'qwiklabs-gcp-2737edcaba61fe20' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the feature columns for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll load the list of categories, authors and article ids we created in the previous **Create Datasets** notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = open(\"categories.txt\").read().splitlines()\n",
    "authors_list = open(\"authors.txt\").read().splitlines()\n",
    "content_ids_list = open(\"content_ids.txt\").read().splitlines()\n",
    "mean_months_since_epoch = 523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stars & Kultur', 'News', 'Lifestyle']\n",
      "['Christina Michlits', 'Mathias Kainz', 'Thomas  Trescher', 'Stefan Berndl', 'Anita Kattinger', 'Martina Salomon', 'Marlene Patsalidis', 'Georg Leyrer', 'Elisabeth Spitzer', 'Elisabeth Sereda', 'Gabriele Kuhn', 'Maria Zelenko', 'Elisabeth Mittendorfer', 'Cordula Puchwein', 'Daniela Wahl', 'Kid Möchel', 'Yvonne Widler', 'Moritz Gottsauner-Wolf', 'Stefan Hofer', 'Raffaela Lindorfer', 'Peter Temel', 'Wolfgang Atzenhofer', 'Heidi Strobl', 'Helmut Brandstätter', 'Sandra Lumetsberger', 'Alexander Huber', 'Mirad Odobasic', 'Irmgard Kischko', 'Daniela Davidovits', 'Bernhard Gaul', 'Ute Brühl', 'Margaretha Kopeinig', 'Ricardo Peyerl', 'Christine Klafl', 'Alexander Strecha', 'Julia Schrenk', 'Michaela Reibenwein', 'Stefanie Rachbauer', 'Hermann Sileitsch-Parzer', 'Andreas Anzenberger', 'Elisabeth Holzer', 'Franz Jandrasits', 'Jens Mattern', 'Sandra Baierl', 'Claudia Elmer', 'Günther Pavlovics', 'Bilal Baltaci', 'Brigitte Schokarth', 'Julia Pfligl', 'Bernhard Ichner', 'Matthias Hofer', 'Heinz Wagner', 'Hedwig Derka', 'Simone Hoepke', 'Werner Rosenberger', 'Thomas Orovits', 'Peter Draxler', 'Tom Schaffer', 'Philipp Albrechtsberger', 'Christoph Geiler', 'Karl Oberascher', 'Florian Holzer', 'Jürgen Zahrl', 'Michael Bachner', 'Caroline Kaltenreiner', 'Thomas Trenkler', 'Armin Arbeiter', 'Christoph Silber', 'Peter Pisa', 'Michael Huber', 'Lisa Rieger', 'Sabine Edelbacher', 'Thomas Sendlhofer', 'Marco Weise', 'Ursula Horvath', 'Michael Pammesberger', 'Michael Andrusio', 'Caecilia Smekal', 'Leila Al-Serori', 'Natascha Marakovits', 'Ingrid Teufl', 'Stefan Kaltenbrunner', 'Wolfgang Winheim', 'Roland Pittner', 'Guido Tartarotti', 'Katharina Zach', 'Johannes Weichhart', 'Klaus Knittelfelder', 'Christian Willim', 'Thomas Martinz', 'Jürgen Klatzer', 'Anita Staudacher', 'Lisa Trompisch', 'Georg Hönigsberger', 'Susanne Mauthner-Weber', 'Konrad Kramar', 'Josef Votzi', 'Stefan Schocher', 'Maria Gurmann', 'Anna-Maria Bauer', 'Johanna Hager', 'Stephan Blumenschein', 'Peter Jarolin', 'Ida Metzger', 'Christian Böhmer', 'Josef Gebhard', 'Johanna Ferner', 'Andrea Hodoschek', 'Christian Schwarz', 'Daniela Kittner', 'Johanna Kreid', 'Radoslaw Zak', 'Dieter Chmelar', 'Dominik Schreiber', 'Patrick Wammerl', 'Laila Daneshmandi', 'Gilbert Weisbier', 'Monika Payreder', 'Karin Leitner', 'Robert Kleedorfer', 'Birgit  Seiser', 'Martin Kubesch', 'Uwe Mauch', 'Maria Kern', 'Daniel Melcher', 'Josef Ertl', 'Evelyn Peternel', 'Andrea Hlinka', 'Philipp Wilhelmer', 'Mathias Morscher', 'Thomas Pressberger', 'Georg Gesellmann', 'Stefan Straka', 'Ernst Mauritz', 'Danny Leder', 'Jasmin Schakfeh', 'Ela Angerer', 'Barbara Kaufmann', 'Nihad Amara', 'Andreas Schwarz', 'Marie North', 'Dirk Hautkapp', 'Jonas Müller', 'Tanja Teufel', 'Elisabeth Gerstendorfer', 'Ingrid Steiner-Gashi', 'Claudia Koglbauer-Schöll', 'Franz Eder', 'Georg Markus', 'Christian Seiler', 'Alexandra Seibel', 'Nina Ellend', 'Birgit Gehrke', 'Daniela Schimke', 'Michael Berger', 'Katharina Salzer', 'Patricia Haller', 'Polly Adler', 'Bernhard Hanisch', 'Gert Korentschnig', 'Alice Hohl', 'Christoph Weiermair', 'Elias Natmessnig', 'Ulrike Botzenhart', 'Marcel Ludwig', 'Annemarie Josef', 'Susanne Bobek', 'Elisabeth  Hofer', 'Theresa Kopper', 'Harald Ottawa', 'Admin I', 'Barbara Stieger', 'Ulla Grünbacher', 'Niki Glattauer', 'Vea Kaiser', 'Andreas Brandt', 'Axel Halbhuber', 'Martin Burger', 'Florian Plavec', 'Dorothe Rainer', 'Julia Gschmeidler', 'Katharina Sunk', 'Theresa Gsellmann', 'Andreas Bovelino', 'Peter Karlik', 'Kim Son Hoang', 'Katharina Wendl', 'Rita Pohler', 'Jürgen Schmidt', 'Florentina Welley', 'Bernhard Praschl', 'Tex Rubinowitz', 'Michael Petermair', 'Barbara Mader', 'Marion Hauser', 'Ernst Molden', 'Peter  Gruber', 'Sissy Rabl', 'Daniel Voglhuber', 'Ulrike Jantschner', 'Stella Reinhold', 'Walter Friedl', 'Philipp Hacker-Walton', 'Stefan Probst', 'Birgit Braunrath', 'Katharina Baier', 'Barbara Reiter', 'Daniela Schmoll', 'Magdalena Vachova', 'Nicole Kolisch', 'Ingrid Bahrer-Fellner', 'Jürgen Pachner', 'Michael Jäger', 'Melanie Raidl', 'Alexandra Uccusic', 'Dietmar Kuss', 'Andreas Heidenreich', 'Christine Scharfetter', 'Anna Hlawatsch', 'Eva Gogala', 'Wilhelm Theuretsbacher', 'Sonja Grundtner', 'Pilar Ortega-Sánchez', 'Alexandra Eder', 'Irene Mayer-Kilani', 'Paul Batruel', 'Anna Gasteiger', 'Christa Schimper', 'Ro Raftl', 'Martin Gantner', 'Carina Tichy', 'Josef Siffert', 'Georg  Haas', 'Christina Pertl', 'Mario Kopf', 'Ricarda Kargl', 'Markus Stingl', 'Markus Foschum', 'Hannes Uhl', 'Brigitte R. Winkler', 'Michael Nowak', 'Andreas Macho', 'Isabella Klausnitzer', 'Stefan Sailer', 'Dominik Sinnreich', 'Otto Klambauer', 'Pascal Sperger', 'Karoline Krause', 'Marie-Therese Tropsch', 'Martin Bernert', 'Robert Rotifer', 'Fiona Köllner', 'Katharina  Fischer', 'Nicholas Bukovec', 'Pedram Seidi', 'Daniel Scheiblberger', 'Julia Karzel', 'Elke Windisch', 'Valerie Pechhacker', 'Michael Horowitz', 'Ankica Nikolić', 'Hubert Huber', 'Brigitta Luchscheider', 'Katharina Fronius', 'Thomas Prenner', 'Paul Schmidt', 'Michael Brandtner', 'Michaela Mottinger', 'Nina Rada', 'Susanne Eiweck', 'Peter Vogl', 'Susanne Lintl', 'Martin Grath', 'Heike Kroemer', 'Jana Patsch', 'Irene Thierjung', 'Rudolf Cijan', 'Veronika Franz', 'Dennis Hader', 'Brigitte Kirchgatterer', 'Andreas Slatner', 'Manfred Imre', 'Claudia Stelzel-Pröll', 'Isabella Radich', 'Nina Horcher', 'Paul Trummer', 'Reinhard Frauscher', 'Christina Pausackl', 'Katrin Wiesinger', 'Stefan Sigwarth', 'Michael Graswald', 'Doris Knecht', 'Marie Achternbusch', 'Teresa Richter-Trummer', 'Jakob Steinschaden', 'Gerhard Krause', 'Niki Nussbaumer', 'Miriam  Steiner', 'Hans Jungbluth', 'Florian Christof', 'Lisa Arnold', 'Thomas Schwantzer', 'Ricarda Lassy', 'Franz Gruber', 'David Kotrba', 'Michael Leitner', 'Jeannine-Beatrice Riepl', 'Philip Mader', 'Tina Deschu', 'Benjamin Sterbenz', 'Lisa Stadler', 'Stefan  Lessmann', 'Norbert Jessen', 'Konstanze Pawlik', 'Gertraud Walch', 'Robert Slovacek', 'Pia Seiser', 'Karl Hohenlohe', 'Victoria Hehle', 'Julia Fenyvesi', 'KURIER Leser/innen', 'Michael Hufnagl', 'Anna Scheutz', 'Sophie Kronberger', 'Barbara Wimmer', 'Belinda Fiebiger', 'Harald Schume', 'Karl-Heinz Jeller', 'Philipp Kienzl', 'Harry Gangl &  Sieglinde Spanlang', 'Tobias Schmitzberger', 'Jürgen Preusser', 'Claudia Zettel', 'Sabine Haag', 'Mailin Treitler', 'Martin Schenk', 'Katharina Peyerl', 'Dragana Heiermann', 'Daniel Lemberger', 'Theresa Herzog', 'Markus Reiter', 'Leopold Stieger', 'Manfred Neuberger', 'Lydia Sprinzl', 'Erich Vogl', 'Patrick Wollner', 'Irina Salewski', 'Stefan Schett', 'Oliver Scheiber', 'Ernst Bieber', 'Barbara Bäre', 'Günter Schmid', 'Lorenz Gallmetzer', 'Claude Brauchbar', 'Werenfried Ressl', 'Manuela Eber', 'Maria Brandl', 'Hubertus Hohenlohe', 'Silvia Kluck', 'Lukas Semmler', 'Matthias Stecher', 'Andrea Punz', 'Josef Schmidt', 'Andrea Wicho', 'Jean-Claude Juncker', 'Isabella Zins', 'Gerald Reischl', 'Marlene Penz', 'Gabriella Haller-Gallee', 'Markus Keßler', 'Harald Eggenberger', 'Magdalena Harmat', 'Benedikt Stimmer', 'Julia Kantner', 'Klemens Riegler-Picker', 'Herbert Gartner', 'Wilhelm Wurm', 'Maria Haiderer', 'Nina Scholz', 'Stefan Binder', 'Theresa Bittermann', 'Jürg Christandl', 'Christoph Heshmatpour', 'Shirin Yanni', 'Martin Stepanek']\n"
     ]
    }
   ],
   "source": [
    "print(categories_list)\n",
    "print(authors_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we'll define the feature columns to use in our model. If necessary, remind yourself the [various feature columns](https://www.tensorflow.org/api_docs/python/tf/feature_column) to use.  \n",
    "For the embedded_title_column feature column, use a Tensorflow Hub Module to create an embedding of the article title. Since the articles and titles are in German, you'll want to use a German language embedding module.  \n",
    "Explore the text embedding Tensorflow Hub modules [available here](https://alpha.tfhub.dev/). Filter by setting the language to 'German'. The 50 dimensional embedding should be sufficient for our purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use a Tensorflow Hub module to create a text embeddding column for the article \"title\". \n",
    "                        # Use the module available at https://alpha.tfhub.dev/ filtering by German language.\n",
    "# this is some crazy pre-trainied embedding model to find similarities in setences and return numeric feature embeddings\n",
    "embedded_title_column = hub.text_embedding_column(key='title'\n",
    "                                                 ,module_spec=\"https://tfhub.dev/google/nnlm-de-dim50/1\"\n",
    "                                                 ,trainable=False)\n",
    "    \n",
    "    \n",
    "#TODO: create an embedded categorical feature column for the article id; i.e. \"content_id\".\n",
    "# First make a \n",
    "embedded_content_column = \n",
    "\n",
    "#TODO: create an embedded categorical feature column for the article \"author\"\n",
    "embedded_author_column = \n",
    "\n",
    "#TODO: create a categorical feature column for the article \"category\"\n",
    "category_column = \n",
    "\n",
    "months_since_epoch_boundaries = list(range(400,700,20))\n",
    "months_since_epoch_bucketized = #TODO: create a bucketized numeric feature column of values for the \"months since epoch\"\n",
    "\n",
    "crossed_months_since_category_column = #TODO: create a crossed feature column using the \"category\" and \"months since epoch\" values\n",
    "\n",
    "feature_columns = [embedded_content_column,\n",
    "                   embedded_author_column,\n",
    "                   category_column,\n",
    "                   embedded_title_column,\n",
    "                   crossed_months_since_category_column] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input function.\n",
    "\n",
    "Next we'll create the input function for our model. This input function reads the data from the csv files we created in the previous labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_defaults = [[\"Unknown\"], [\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[mean_months_since_epoch],[\"Unknown\"]]\n",
    "column_keys = [\"visitor_id\", \"content_id\", \"category\", \"title\", \"author\", \"months_since_epoch\", \"next_content_id\"]\n",
    "label_key = \"next_content_id\"\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "      def decode_csv(value_column):\n",
    "          columns = tf.decode_csv(value_column,record_defaults=record_defaults)\n",
    "          features = dict(zip(column_keys, columns))          \n",
    "          label = features.pop(label_key)         \n",
    "          return features, label\n",
    "\n",
    "      # Create list of files that match pattern\n",
    "      file_list = tf.gfile.Glob(filename)\n",
    "\n",
    "      # Create dataset from file list\n",
    "      dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "          num_epochs = None # indefinitely\n",
    "          dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "      else:\n",
    "          num_epochs = 1 # end-of-input after this\n",
    "\n",
    "      dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "      return dataset.make_one_shot_iterator().get_next()\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model and train/evaluate\n",
    "\n",
    "\n",
    "Next, we'll build our model which recommends an article for a visitor to the Kurier.at website. Look through the code below. We use the input_layer feature column to create the dense input layer to our network. This is just a sigle layer network where we can adjust the number of hidden units as a parameter.\n",
    "\n",
    "Currently, we compute the accuracy between our predicted 'next article' and the actual 'next article' read next by the visitor. Resolve the TODOs in the cell below by adding additional performance metrics to assess our model. You will need to \n",
    "* use the [tf.metrics library](https://www.tensorflow.org/api_docs/python/tf/metrics) to compute an additional performance metric\n",
    "* add this additional metric to the metrics dictionary, and \n",
    "* include it in the tf.summary that is sent to Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "  net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "  for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "   # Compute logits (1 per class).\n",
    "  logits = tf.layers.dense(net, params['n_classes'], activation=None) \n",
    "\n",
    "  predicted_classes = tf.argmax(logits, 1)\n",
    "  from tensorflow.python.lib.io import file_io\n",
    "    \n",
    "  with file_io.FileIO('content_ids.txt', mode='r') as ifp:\n",
    "    content = tf.constant([x.rstrip() for x in ifp])\n",
    "  predicted_class_names = tf.gather(content, predicted_classes)\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    predictions = {\n",
    "        'class_ids': predicted_classes[:, tf.newaxis],\n",
    "        'class_names' : predicted_class_names[:, tf.newaxis],\n",
    "        'probabilities': tf.nn.softmax(logits),\n",
    "        'logits': logits,\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "  table = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"content_ids.txt\")\n",
    "  labels = table.lookup(labels)\n",
    "  # Compute loss.\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Compute evaluation metrics.\n",
    "  accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                 predictions=predicted_classes,\n",
    "                                 name='acc_op')\n",
    "  top_10_accuracy = #TODO: Compute the top_10 accuracy, using the tf.nn.in_top_k and tf.metrics.mean functions in Tensorflow\n",
    "  \n",
    "  metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    #TODO: Add top_10_accuracy to the metrics dictionary\n",
    "  }\n",
    "  \n",
    "  tf.summary.scalar('accuracy', accuracy[1])\n",
    "  #TODO: Add the top_10_accuracy metric to the Tensorboard summary\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "      return tf.estimator.EstimatorSpec(\n",
    "          mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "  # Create training op.\n",
    "  assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'content_based_model_trained'\n",
    "shutil.rmtree(outdir, ignore_errors = True) # start fresh each time\n",
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir = outdir,\n",
    "    params={\n",
    "     'feature_columns': feature_columns,\n",
    "      'hidden_units': [200, 100, 50],\n",
    "      'n_classes': len(content_ids_list)\n",
    "    })\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = read_dataset(\"training_set.csv\", tf.estimator.ModeKeys.TRAIN),\n",
    "    max_steps = 200)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = read_dataset(\"test_set.csv\", tf.estimator.ModeKeys.EVAL),\n",
    "    steps = None,\n",
    "    start_delay_secs = 30,\n",
    "    throttle_secs = 60)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with the trained model. \n",
    "\n",
    "With the model now trained, we can make predictions by calling the predict method on the estimator. Let's look at how our model predicts on the first five examples of the training set.  \n",
    "To start, we'll create a new file 'first_5.csv' which contains the first five elements of our training set. We'll also save the target values to a file 'first_5_content_ids' so we can compare our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -5 training_set.csv > first_5.csv\n",
    "head first_5.csv\n",
    "awk -F \"\\\"*,\\\"*\" '{print $2}' first_5.csv > first_5_content_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, to make predictions on the trained model we pass a list of examples through the input function. Complete the code below to make predicitons on the examples contained in the \"first_5.csv\" file we created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = #TODO: Use the predict method on our trained model to find the predictions for the examples contained in \"first_5.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "recommended_content_ids = [np.asscalar(d[\"class_names\"]).decode('UTF-8') for d in output]\n",
    "content_ids = open(\"first_5_content_ids\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll map the content id back to the article title. We can then compare our model's recommendation for the first of our examples. This can all be done in BigQuery. Look through the query below and make sure it is clear what is being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "recommended_title_sql=\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "(SELECT MAX(IF(index=6, value, NULL)) FROM UNNEST(hits.customDimensions)) AS title\n",
    "FROM `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
    "  UNNEST(hits) AS hits\n",
    "WHERE \n",
    "  # only include hits on pages\n",
    "  hits.type = \"PAGE\"\n",
    "  AND (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) = \\\"{}\\\"\n",
    "LIMIT 1\"\"\".format(recommended_content_ids[0])\n",
    "\n",
    "current_title_sql=\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "(SELECT MAX(IF(index=6, value, NULL)) FROM UNNEST(hits.customDimensions)) AS title\n",
    "FROM `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
    "  UNNEST(hits) AS hits\n",
    "WHERE \n",
    "  # only include hits on pages\n",
    "  hits.type = \"PAGE\"\n",
    "  AND (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) = \\\"{}\\\"\n",
    "LIMIT 1\"\"\".format(content_ids[0])\n",
    "recommended_title = bq.Query(recommended_title_sql).execute().result().to_dataframe()['title'].tolist()[0]\n",
    "current_title = bq.Query(current_title_sql).execute().result().to_dataframe()['title'].tolist()[0]\n",
    "print(\"Current title: {} \".format(current_title))\n",
    "print(\"Recommended title: {}\".format(recommended_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "As usual, we can monitor the performance of our training job using Tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('content_based_model_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print(\"Stopped TensorBoard with pid {}\".format(pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
